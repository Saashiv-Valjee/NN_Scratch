{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad2d28b9",
   "metadata": {},
   "source": [
    "# Back Propogation\n",
    "\n",
    "The idea is that we want to tune the weights such that our loss points travel down to the global minima within the loss space over iterations of learning. To do this we must formulate how the inputs relate to the loss. This is done via the loss gradient, expressed as \n",
    "\n",
    "$$\n",
    "\\nabla \\mathcal{L} = [\\frac{\\partial C}{\\partial W^i},\\frac{\\partial C}{\\partial b^i}]\n",
    "$$\n",
    "\n",
    "where the index refers to a matrix representing the weights or bias for an entire layer, where its location (subscript or superscript) is non important "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a240a1",
   "metadata": {},
   "source": [
    "# $\\frac{\\partial C}{\\partial W^i}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b91369",
   "metadata": {},
   "source": [
    "## Derivation of Binary Cross Entropy Gradient\n",
    "\n",
    "We start with the loss function:\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = \\frac{1}{M} \\sum_{i=1}^{M} -\\left( y_i \\ln(\\hat{y}_i) + (1 - y_i) \\ln(1 - \\hat{y}_i) \\right)\n",
    "$$\n",
    "\n",
    "**(Defined binary cross entropy loss averaged over $M$ samples)**\n",
    "\n",
    "\n",
    "\n",
    "The network output at node $i$ is given by:\n",
    "\n",
    "$$\n",
    "\\hat{y}_i = \\text{act}_2(z_i)\n",
    "$$\n",
    "\n",
    "**(Defined the activation output at layer $i$)**\n",
    "\n",
    "\n",
    "\n",
    "And the pre-activation is:\n",
    "\n",
    "$$\n",
    "z_i = A_{i-1} W_i + B_i\n",
    "$$\n",
    "\n",
    "**(Defined $z_i$ as the affine transformation of the previous layer)**\n",
    "\n",
    "\n",
    "\n",
    "We want the derivative of the loss with respect to $W_i$:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial W_i}\n",
    "$$\n",
    "\n",
    "**(Stated our backpropagation objective)**\n",
    "\n",
    "\n",
    "\n",
    "We have:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial z_i}{\\partial W_i}, \\quad \\frac{\\partial \\hat{y}_i}{\\partial z_i}, \\quad \\frac{\\partial \\mathcal{L}}{\\partial \\hat{y}_i}\n",
    "$$\n",
    "\n",
    "**(Listed the components needed via the chain rule)**\n",
    "\n",
    "\n",
    "\n",
    "Using the chain rule:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial W_i} = \\frac{\\partial \\mathcal{L}}{\\partial \\hat{y}_i} \\cdot \\frac{\\partial \\hat{y}_i}{\\partial z_i} \\cdot \\frac{\\partial z_i}{\\partial W_i}\n",
    "$$\n",
    "\n",
    "**(Applied the chain rule to compute $\\frac{\\partial \\mathcal{L}}{\\partial W_i}$)**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8fd8ffb",
   "metadata": {},
   "source": [
    "# $\\frac{\\partial \\mathcal{L}}{\\partial \\hat{y}_i}$\n",
    "\n",
    "We begin with the binary cross entropy loss function:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\hat{y}, y) = -\\left[ y \\log(\\hat{y}) + (1 - y) \\log(1 - \\hat{y}) \\right]\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "### Step 1: Differentiate $\\mathcal{L}$ with respect to $\\hat{y}$\n",
    "\n",
    "We apply the derivative term-by-term:\n",
    "\n",
    "$$\n",
    "\\frac{d\\mathcal{L}}{d\\hat{y}} = \\frac{d}{d\\hat{y}} \\left[ - y \\log(\\hat{y}) - (1 - y) \\log(1 - \\hat{y}) \\right]\n",
    "$$\n",
    "\n",
    "**(Expanded the negative sign to both terms)**\n",
    "\n",
    "\n",
    "\n",
    "### Step 2: Differentiate the first term\n",
    "\n",
    "$$\n",
    "\\frac{d}{d\\hat{y}} \\left[ - y \\log(\\hat{y}) \\right] = - y \\cdot \\frac{1}{\\hat{y}}\n",
    "$$\n",
    "\n",
    "**(Used derivative $\\frac{d}{dx} \\log(x) = \\frac{1}{x}$)**\n",
    "\n",
    "\n",
    "\n",
    "### Step 3: Differentiate the second term\n",
    "\n",
    "$$\n",
    "\\frac{d}{d\\hat{y}} \\left[ - (1 - y) \\log(1 - \\hat{y}) \\right] = - (1 - y) \\cdot \\left( \\frac{-1}{1 - \\hat{y}} \\right)\n",
    "$$\n",
    "\n",
    "**(Applied chain rule: derivative of $\\log(1 - \\hat{y})$ is $-\\frac{1}{1 - \\hat{y}}$)**\n",
    "\n",
    "\n",
    "\n",
    "### Step 4: Simplify the second derivative\n",
    "\n",
    "$$\n",
    "- (1 - y) \\cdot \\left( \\frac{-1}{1 - \\hat{y}} \\right) = \\frac{1 - y}{1 - \\hat{y}}\n",
    "$$\n",
    "\n",
    "**(Simplified negative signs)**\n",
    "\n",
    "\n",
    "\n",
    "### Step 5: Combine both derivative terms\n",
    "\n",
    "$$\n",
    "\\frac{d\\mathcal{L}}{d\\hat{y}} = - \\frac{y}{\\hat{y}} + \\frac{1 - y}{1 - \\hat{y}}\n",
    "$$\n",
    "\n",
    "**(Substituted results from Step 2 and Step 4 into Step 1)**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868550e3",
   "metadata": {},
   "source": [
    "# $\\frac{\\partial \\hat{y}_i}{\\partial z_i}$\n",
    "\n",
    "We start with the definition of the sigmoid function:\n",
    "\n",
    "$$\n",
    "\\hat{y}_i = \\frac{1}{1 + e^{-z}}\n",
    "$$\n",
    "\n",
    "**(Defined the sigmoid activation function)**\n",
    "\n",
    "\n",
    "\n",
    "We rewrite this with a negative exponent:\n",
    "\n",
    "$$\n",
    "\\hat{y}_i = (1 + e^{-z})^{-1}\n",
    "$$\n",
    "\n",
    "**(Rewrote the expression using negative exponent notation)**\n",
    "\n",
    "\n",
    "\n",
    "Now take the derivative using the chain rule:\n",
    "\n",
    "$$\n",
    "\\frac{d\\sigma}{dz} = -1 \\cdot (1 + e^{-z})^{-2} \\cdot \\left(-e^{-z}\\right)\n",
    "$$\n",
    "\n",
    "**(Applied the chain rule: outer function is power, inner is exponential)**\n",
    "\n",
    "\n",
    "\n",
    "Simplify the negatives:\n",
    "\n",
    "$$\n",
    "\\frac{d\\sigma}{dz} = \\frac{e^{-z}}{(1 + e^{-z})^2}\n",
    "$$\n",
    "\n",
    "**(Simplified the signs in the numerator)**\n",
    "\n",
    "\n",
    "\n",
    "Now express numerator and denominator in terms of $\\hat{y}_i$:\n",
    "\n",
    "Recall:\n",
    "- $\\hat{y}_i = \\frac{1}{1 + e^{-z}}$\n",
    "- $1 - \\hat{y}_i = \\frac{e^{-z}}{1 + e^{-z}}$\n",
    "\n",
    "Multiply them:\n",
    "\n",
    "$$\n",
    "\\frac{d\\sigma}{dz} = \\hat{y}_i(1 - \\hat{y}_i)\n",
    "$$\n",
    "\n",
    "**(Rewrote the derivative in terms of $\\hat{y}_i$ only)**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac34ddc",
   "metadata": {},
   "source": [
    "# $\\frac{\\partial z_i}{\\partial W_i}$\n",
    "\n",
    "recall: \n",
    "\n",
    "$$\n",
    "z_i = A_{i-1} W_i + B_i\n",
    "$$\n",
    "\n",
    "The derivative is simply \n",
    "\n",
    "$$\n",
    "\\frac{\\partial z_i}{\\partial W_i} = A_{i-1}^T\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61012a7b",
   "metadata": {},
   "source": [
    "# $\\frac{\\partial \\mathcal{L}}{\\partial W_i}$\n",
    "\n",
    "Recall: \n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial W_i} = \\frac{\\partial \\mathcal{L}}{\\partial \\hat{y}_i} \\cdot \\frac{\\partial \\hat{y}_i}{\\partial z_i} \\cdot \\frac{\\partial z_i}{\\partial W_i}\n",
    "$$\n",
    "\n",
    "Which is now\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial W_i} =  [- \\frac{y}{\\hat{y}} + \\frac{1 - y}{1 - \\hat{y}}][\\hat{y}_i(1 - \\hat{y}_i)]A_{i-1}^T\n",
    "$$\n",
    "\n",
    "Or more compactly known as \n",
    "\n",
    "$$\n",
    "\\frac{1}{m} \\sum (A^i - y)(A^{i-1})^T\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c111fda",
   "metadata": {},
   "source": [
    "# $\\frac{\\partial \\mathcal{L}}{\\partial b_i}$\n",
    "\n",
    "since the bias per layer can be evaluated as the sum of non dependence number, it's derivative is just 1.\n",
    "\n",
    "$$\n",
    "Z^i = W^iA^{i-1} + b_i\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial z^i}{\\partial b^i} = 1\n",
    "$$\n",
    "\n",
    "To obtain $\\frac{\\partial \\mathcal{L}}{\\partial b^i}$ we can use the chain rule\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial b^i} = \\frac{\\partial \\mathcal{L}}{\\partial z^i} \\frac{\\partial z^i}{\\partial b^i}\n",
    "$$\n",
    "\n",
    "using the chain rule  \n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial z_i} = \\frac{\\partial \\mathcal{L}}{\\partial \\hat{y}_i} \\cdot \\frac{\\partial \\hat{y}_i}{\\partial z_i} \n",
    "$$\n",
    "\n",
    "therefore\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial b_i} = \\frac{\\partial \\mathcal{L}}{\\partial \\hat{y}_i} \\cdot \\frac{\\partial \\hat{y}_i}{\\partial z_i} \\cdot \\frac{\\partial z^i}{\\partial b^i}\n",
    "$$\n",
    "\n",
    "finally, we have \n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial b_i} = \\frac{1}{m} \\sum (A^i - y)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b8e7d9",
   "metadata": {},
   "source": [
    "now that we have a way to directly relate the initial weights and biases to the loss function, we can tune the initial weights depending on the way they cause the derivative of the loss function to change! this is gradient descent"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
